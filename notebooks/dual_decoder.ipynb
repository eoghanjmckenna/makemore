{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right so this is exciting, we've gone through Karpathy's lectures, and have trained our first decoder only transformer model that is based on the same architecture as used in ChatGPT. Now we want to start extending this. In our case, we want a similar architecture but to have two decoders in parallel, one for the gas demand data generation, and one for the electricity demand generation. In addition, we want to add a cross-attention block, where each decoder attends to the other sequence.\n",
    "\n",
    "However, a complication arises. The cross attention is traditionally done with an encoder block. And it is the output of the encoder block which is then fed into the cross-attention layer in each layer of the decoder. That means that for the dual decoders to attend to each other, they initially need the equivalent of an encoder only block, or a decode-only block or blocks without cross-attention layers, the output from which can be used in subsequent decoder cross-attention layers. \n",
    "\n",
    "In which case the convenient way to think about this is therefore sequential, this is a dual encoder-decoder transformer, but with these both in sequence i.e. an encoder block followed by a decoder block with cross attention on the other parallel encoder block. One of the consequences, is that the encoder for each stream, will learn the information required for both streams simultaneously. \n",
    "\n",
    "Actually, no, we cannot use encoder blocks for the initial stage, as that would incorporate information from the future, which we cannot have. Therefore it will be sequential decoder only blocks, followed by decoder with masked cross attention to the other initial decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so the structure for each sequential encoder-decoder (of which there will be two in parallel) will be:\n",
    "1. input embedding\n",
    "2. positional embedding\n",
    "3. Initial decoder only block(s)\n",
    "   1. pre-norm\n",
    "   2. multi-head attention (masked)\n",
    "   3. add (residual)\n",
    "   4. pre-norm\n",
    "   5. FF\n",
    "   6. Add (residual)\n",
    "4. decoder with cross attention blocks\n",
    "   1. pre-norm\n",
    "   2. multi-head attention (masked)\n",
    "   3. add (residual)\n",
    "   4. multi-head cross attention (masked) using key, values from initial decoder only block from parallel stream\n",
    "   5. add & norm\n",
    "   6. FF\n",
    "   7. Add & norm\n",
    "5. linear\n",
    "6. softmax\n",
    "7. output probabilities over the vocab_size\n",
    "\n",
    "So, let's start by trying to make one of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 8\n",
    "n_head = 1\n",
    "n_layer_do = 1 # number of decoder only layers\n",
    "n_layer_ca = 1 # number of cross-attention layers\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1338)\n",
    "# get the data\n",
    "data = pd.read_csv('/Users/eoghan/repos/makemore/data/daily_demand_and_weather.csv')\n",
    "# to start, we will just work with the gas data, later we will complexify things by adding in electricity and weather, and information about the calender day\n",
    "raw_gas = data['mean_rounded_gas_pulse'].copy()\n",
    "\n",
    "# set all values less than 0 to 0\n",
    "raw_gas.loc[raw_gas < 0] = 0\n",
    "\n",
    "# set unreasonably high values to Nan\n",
    "raw_gas.loc[raw_gas > 100000] = np.nan\n",
    "\n",
    "# so let's simplify the dataset, and round raw_gas to the nearest 10, just to reduce the size of our vocabulary. \n",
    "raw_gas = raw_gas.round(-1)\n",
    "\n",
    "# first however we need to deal with missing values, in particular we need to replace any nan with a special character <M> which will represent missing values\n",
    "raw_gas = raw_gas.astype(str).replace('nan', '<M>')\n",
    "\n",
    "#create a mapping from values to indices\n",
    "unique_values_gas = raw_gas.unique()\n",
    "vocab_size_gas = len(unique_values_gas)\n",
    "unique_values_gas.sort()\n",
    "vtoi_gas = {val:i for i, val in enumerate(unique_values_gas)}\n",
    "itov_gas = {i:val for i, val in enumerate(unique_values_gas)}\n",
    "encode_gas = lambda v: [vtoi_gas[val] for val in v] # take a list of values and return a list of indices\n",
    "decode_gas = lambda l: [itov_gas[i] for i in l] # take a list of indices and return a list of values\n",
    "\n",
    "data_gas = torch.tensor(encode_gas(raw_gas), dtype=torch.long)\n",
    "\n",
    "# let's split the data into train and validation splits 0.9 / 0.1\n",
    "#n = int(0.9*len(data_gas))\n",
    "n = int(0.1*len(data_gas))\n",
    "train_data_gas = data_gas[n:]\n",
    "val_data_gas = data_gas[:n]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data_gas if split == 'train' else val_data_gas\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedHead(nn.Module):\n",
    "    \"\"\" one head of masked self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossAttentionHead(nn.Module):\n",
    "    \"\"\" one head of masked self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = q.shape\n",
    "        k = self.key(kv)   # (B,T,hs)\n",
    "        q = self.query(q) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(kv) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([MaskedHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([MaskedCrossAttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyBlock(nn.Module):\n",
    "    \"\"\" Decoder only transformer block: self-communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class DecoderWithCrossAttentionBlock(nn.Module):\n",
    "    \"\"\" Decoder with cross attention transformer block: self plus corss communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ca = MultiHeadCrossAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln_kv = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, kv):\n",
    "        # first masked self-attention layer with pre norm and residual connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # cross attention layer with pre norm and residual connection\n",
    "        x = x + self.ca(self.ln2(x), self.ln_kv(kv))\n",
    "        x = x + self.ffwd(self.ln3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualTransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_gas = nn.Embedding(vocab_size_gas, n_embd)\n",
    "        self.token_embedding_elec = nn.Embedding(vocab_size_elec, n_embd)\n",
    "        self.position_embedding_gas = nn.Embedding(block_size, n_embd)\n",
    "        self.position_embedding_elec = nn.Embedding(block_size, n_embd)\n",
    "        self.do_blocks_gas = nn.Sequential(*[DecoderOnlyBlock(n_embd, n_head=n_head) for _ in range(n_layer_do)])\n",
    "        self.do_blocks_elec = nn.Sequential(*[DecoderOnlyBlock(n_embd, n_head=n_head) for _ in range(n_layer_do)])\n",
    "        self.ca_blocks_gas = nn.Sequential(*[DecoderWithCrossAttentionBlock(n_embd, n_head=n_head) for _ in range(n_layer_ca)])\n",
    "        self.ca_blocks_elec = nn.Sequential(*[DecoderWithCrossAttentionBlock(n_embd, n_head=n_head) for _ in range(n_layer_ca)])\n",
    "        self.ln_f_gas = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.ln_f_elec = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head_gas = nn.Linear(n_embd, vocab_size_gas)\n",
    "        self.lm_head_elec = nn.Linear(n_embd, vocab_size_gas)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx_gas, idx_elec, targets_gas=None, targets_elec=None):\n",
    "        B, T = idx_gas.shape # will be the same for idx_elec\n",
    "\n",
    "        # idx_gas and targets_gas are both (B,T) tensor of integers\n",
    "        # embedding and positional encoding layers\n",
    "        tok_emb_gas = self.token_embedding_gas(idx_gas) # (B,T,C)\n",
    "        tok_emb_elec = self.token_embedding_elec(idx_elec) # (B,T,C)\n",
    "        pos_emb_gas = self.position_embedding_gas(torch.arange(T, device=device)) # (T,C)\n",
    "        pos_emb_elec = self.position_embedding_elec(torch.arange(T, device=device)) # (T,C)\n",
    "        x_gas = tok_emb_gas + pos_emb_gas # (B,T,C)\n",
    "        x_elec = tok_emb_elec + pos_emb_elec # (B,T,C)\n",
    "        # decoder only layers\n",
    "        x_gas = self.do_blocks_gas(x_gas) # (B,T,C)\n",
    "        x_elec = self.do_blocks_elec(x_elec) # (B,T,C)\n",
    "        # cross attention layers\n",
    "        x_gas = self.ca_blocks_gas(x_gas, x_elec) # (B,T,C)\n",
    "        x_elec = self.ca_blocks_gas(x_elec, x_gas) # (B,T,C)\n",
    "        # final output layers\n",
    "        x_gas = self.ln_f_gas(x_gas) # (B,T,C)\n",
    "        x_elec = self.ln_f_elec(x_elec) # (B,T,C)\n",
    "        logits_gas = self.lm_head_gas(x_gas) # (B,T,vocab_size)\n",
    "        logits_elec = self.lm_head_elec(x_elec) # (B,T,vocab_size)\n",
    "\n",
    "        if targets_gas is None:\n",
    "            loss_gas = None\n",
    "        else:\n",
    "            B, T, C = logits_gas.shape\n",
    "            logits_gas = logits_gas.view(B*T, C)\n",
    "            targets_gas = targets_gas.view(B*T)\n",
    "            loss_gas = F.cross_entropy(logits_gas, targets_gas)\n",
    "        # now the same for elec\n",
    "        if targets_elec is None:\n",
    "            loss_elec = None\n",
    "        else:\n",
    "            B, T, C = logits_elec.shape\n",
    "            logits_elec = logits_elec.view(B*T, C)\n",
    "            targets_elec = targets_elec.view(B*T)\n",
    "            loss_elec = F.cross_entropy(logits_elec, targets_elec)\n",
    "\n",
    "        return logits_gas, loss_gas, logits_elec, loss_elec\n",
    "\n",
    "    def generate(self, idx_gas, idx_elec, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond_gas = idx_gas[:, -block_size:]\n",
    "            idx_cond_elec = idx_elec[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits_gas, _, logits_elec, _ = self(idx_cond_gas, idx_cond_elec)\n",
    "            # focus only on the last time step\n",
    "            logits_gas = logits_gas[:, -1, :] # becomes (B, C)\n",
    "            logits_elec = logits_elec[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs_gas = F.softmax(logits_gas, dim=-1) # (B, C)\n",
    "            probs_elec = F.softmax(logits_elec, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next_gas = torch.multinomial(probs_gas, num_samples=1) # (B, 1)\n",
    "            idx_next_elec = torch.multinomial(probs_elec, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx_gas = torch.cat((idx_gas, idx_next_gas), dim=1) # (B, T+1)\n",
    "            idx_elec = torch.cat((idx_elec, idx_next_elec), dim=1) # (B, T+1)\n",
    "        return idx_gas, idx_elec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
