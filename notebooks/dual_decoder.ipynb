{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right so this is exciting, we've gone through Karpathy's lectures, and have trained our first decoder only transformer model that is based on the same architecture as used in ChatGPT. Now we want to start extending this. In our case, we want a similar architecture but to have two decoders in parallel, one for the gas demand data generation, and one for the electricity demand generation. In addition, we want to add a cross-attention block, where each decoder attends to the other sequence.\n",
    "\n",
    "However, a complication arises. The cross attention is traditionally done with an encoder block. And it is the output of the encoder block which is then fed into the cross-attention layer in each layer of the decoder. That means that for the dual decoders to attend to each other, they initially need the equivalent of an encoder only block, or a decode-only block or blocks without cross-attention layers, the output from which can be used in subsequent decoder cross-attention layers. \n",
    "\n",
    "In which case the convenient way to think about this is therefore sequential, this is a dual encoder-decoder transformer, but with these both in sequence i.e. an encoder block followed by a decoder block with cross attention on the other parallel encoder block. One of the consequences, is that the encoder for each stream, will learn the information required for both streams simultaneously. \n",
    "\n",
    "Actually, no, we cannot use encoder blocks for the initial stage, as that would incorporate information from the future, which we cannot have. Therefore it will be sequential decoder only blocks, followed by decoder with masked cross attention to the other initial decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so the structure for each sequential encoder-decoder (of which there will be two in parallel) will be:\n",
    "1. input embedding\n",
    "2. positional embedding\n",
    "3. Initial decoder only block(s)\n",
    "   1. pre-norm\n",
    "   2. multi-head attention (masked)\n",
    "   3. add (residual)\n",
    "   4. pre-norm\n",
    "   5. FF\n",
    "   6. Add (residual)\n",
    "4. decoder with cross attention blocks\n",
    "   1. pre-norm\n",
    "   2. multi-head attention (masked)\n",
    "   3. add (residual)\n",
    "   4. multi-head cross attention (masked) using key, values from initial decoder only block from parallel stream\n",
    "   5. add & norm\n",
    "   6. FF\n",
    "   7. Add & norm\n",
    "5. linear\n",
    "6. softmax\n",
    "7. output probabilities over the vocab_size\n",
    "\n",
    "So, let's start by trying to make one of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 8\n",
    "n_head = 1\n",
    "n_layer_do = 1 # number of decoder only layers\n",
    "n_layer_ca = 1 # number of cross-attention layers\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1338)\n",
    "# get the data\n",
    "data = pd.read_csv('/Users/eoghan/repos/makemore/data/daily_demand_and_weather.csv')\n",
    "# to start, we will just work with the gas data, later we will complexify things by adding in electricity and weather, and information about the calender day\n",
    "raw_gas = data['mean_rounded_gas_pulse'].copy()\n",
    "raw_elec = data['mean_rounded_electric_combined'].copy()\n",
    "\n",
    "# set all values less than 0 to 0\n",
    "raw_gas.loc[raw_gas < 0] = 0\n",
    "raw_elec.loc[raw_elec < 0] = 0\n",
    "\n",
    "# set unreasonably high values to Nan\n",
    "raw_gas.loc[raw_gas > 100000] = np.nan\n",
    "raw_elec.loc[raw_elec > 100000] = np.nan\n",
    "\n",
    "# so let's simplify the dataset, and round raw_gas to the nearest 10, just to reduce the size of our vocabulary. \n",
    "raw_gas = raw_gas.round(-1)\n",
    "raw_elec = raw_elec.round(-1)\n",
    "\n",
    "# first however we need to deal with missing values, in particular we need to replace any nan with a special character <M> which will represent missing values\n",
    "raw_gas = raw_gas.astype(str).replace('nan', '<M>')\n",
    "raw_elec = raw_elec.astype(str).replace('nan', '<M>')\n",
    "\n",
    "#create a mapping from values to indices\n",
    "unique_values_gas = raw_gas.unique()\n",
    "vocab_size_gas = len(unique_values_gas)\n",
    "unique_values_gas.sort()\n",
    "vtoi_gas = {val:i for i, val in enumerate(unique_values_gas)}\n",
    "itov_gas = {i:val for i, val in enumerate(unique_values_gas)}\n",
    "encode_gas = lambda v: [vtoi_gas[val] for val in v] # take a list of values and return a list of indices\n",
    "decode_gas = lambda l: [itov_gas[i] for i in l] # take a list of indices and return a list of values\n",
    "data_gas = torch.tensor(encode_gas(raw_gas), dtype=torch.long)\n",
    "\n",
    "# let's split the data into train and validation splits 0.9 / 0.1\n",
    "#n = int(0.9*len(data_gas))\n",
    "n = int(0.1*len(data_gas))\n",
    "train_data_gas = data_gas[n:]\n",
    "val_data_gas = data_gas[:n]\n",
    "\n",
    "# and let's do the same for raw_elec\n",
    "unique_values_elec = raw_elec.unique()\n",
    "vocab_size_elec = len(unique_values_elec)\n",
    "unique_values_elec.sort()\n",
    "vtoi_elec = {val:i for i, val in enumerate(unique_values_elec)}\n",
    "itov_elec = {i:val for i, val in enumerate(unique_values_elec)}\n",
    "encode_elec = lambda v: [vtoi_elec[val] for val in v] # take a list of values and return a list of indices\n",
    "decode_elec = lambda l: [itov_elec[i] for i in l] # take a list of indices and return a list of values\n",
    "data_elec = torch.tensor(encode_elec(raw_elec), dtype=torch.long)\n",
    "n = int(0.1*len(data_elec))\n",
    "train_data_elec = data_elec[n:]\n",
    "val_data_elec = data_elec[:n]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y for gas\n",
    "    data = train_data_gas if split == 'train' else val_data_gas\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x_gas = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y_gas = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x_gas, y_gas = x_gas.to(device), y_gas.to(device)\n",
    "    # now do the same for elec\n",
    "    data = train_data_elec if split == 'train' else val_data_elec\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x_elec = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y_elec = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x_elec, y_elec = x_elec.to(device), y_elec.to(device)\n",
    "    return x_gas, y_gas, x_elec, y_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters, 2)\n",
    "        for k in range(eval_iters):\n",
    "            X_gas, Y_gas, X_elec, Y_elec = get_batch(split)\n",
    "            logits_gas, loss_gas, logits_elec, loss_elec = model(X_gas, X_elec, Y_gas, Y_elec)\n",
    "            losses[k,1] = loss_gas.item()\n",
    "            losses[k,2] = loss_elec.item()\n",
    "        out[split] = torch.concat([losses[:,0].mean(), losses[:,1].mean()], dim=0)\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedHead(nn.Module):\n",
    "    \"\"\" one head of masked self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossAttentionHead(nn.Module):\n",
    "    \"\"\" one head of masked cross-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = q.shape\n",
    "        k = self.key(kv)   # (B,T,hs)\n",
    "        q = self.query(q) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(kv) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([MaskedHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([MaskedCrossAttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, kv):\n",
    "        out = torch.cat([h(x, kv) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyBlock(nn.Module):\n",
    "    \"\"\" Decoder only transformer block: self-communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class DecoderWithCrossAttentionBlock(nn.Module):\n",
    "    \"\"\" Decoder with cross attention transformer block: self plus corss communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ca = MultiHeadCrossAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln_kv = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, kv):\n",
    "        # first masked self-attention layer with pre norm and residual connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # cross attention layer with pre norm and residual connection\n",
    "        x = x + self.ca(self.ln2(x), self.ln_kv(kv))\n",
    "        x = x + self.ffwd(self.ln3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualTransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_gas = nn.Embedding(vocab_size_gas, n_embd)\n",
    "        self.token_embedding_elec = nn.Embedding(vocab_size_elec, n_embd)\n",
    "        self.position_embedding_gas = nn.Embedding(block_size, n_embd)\n",
    "        self.position_embedding_elec = nn.Embedding(block_size, n_embd)\n",
    "        self.do_blocks_gas = nn.Sequential(*[DecoderOnlyBlock(n_embd, n_head=n_head) for _ in range(n_layer_do)])\n",
    "        self.do_blocks_elec = nn.Sequential(*[DecoderOnlyBlock(n_embd, n_head=n_head) for _ in range(n_layer_do)])\n",
    "        self.ca_blocks_gas = nn.Sequential(*[DecoderWithCrossAttentionBlock(n_embd, n_head=n_head) for _ in range(n_layer_ca)])\n",
    "        self.ca_blocks_elec = nn.Sequential(*[DecoderWithCrossAttentionBlock(n_embd, n_head=n_head) for _ in range(n_layer_ca)])\n",
    "        self.ln_f_gas = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.ln_f_elec = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head_gas = nn.Linear(n_embd, vocab_size_gas)\n",
    "        self.lm_head_elec = nn.Linear(n_embd, vocab_size_gas)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx_gas, idx_elec, targets_gas=None, targets_elec=None):\n",
    "        B, T = idx_gas.shape # will be the same for idx_elec\n",
    "\n",
    "        # idx_gas and targets_gas are both (B,T) tensor of integers\n",
    "        # embedding and positional encoding layers\n",
    "        tok_emb_gas = self.token_embedding_gas(idx_gas) # (B,T,C)\n",
    "        tok_emb_elec = self.token_embedding_elec(idx_elec) # (B,T,C)\n",
    "        pos_emb_gas = self.position_embedding_gas(torch.arange(T, device=device)) # (T,C)\n",
    "        pos_emb_elec = self.position_embedding_elec(torch.arange(T, device=device)) # (T,C)\n",
    "        x_gas = tok_emb_gas + pos_emb_gas # (B,T,C)\n",
    "        x_elec = tok_emb_elec + pos_emb_elec # (B,T,C)\n",
    "        # decoder only layers\n",
    "        x_gas = self.do_blocks_gas(x_gas) # (B,T,C)\n",
    "        x_elec = self.do_blocks_elec(x_elec) # (B,T,C)\n",
    "        # cross attention layers\n",
    "        x_gas = self.ca_blocks_gas(x_gas, x_elec) # (B,T,C)\n",
    "        x_elec = self.ca_blocks_elec(x_elec, x_gas) # (B,T,C)\n",
    "        # final output layers\n",
    "        x_gas = self.ln_f_gas(x_gas) # (B,T,C)\n",
    "        x_elec = self.ln_f_elec(x_elec) # (B,T,C)\n",
    "        logits_gas = self.lm_head_gas(x_gas) # (B,T,vocab_size)\n",
    "        logits_elec = self.lm_head_elec(x_elec) # (B,T,vocab_size)\n",
    "\n",
    "        if targets_gas is None:\n",
    "            loss_gas = None\n",
    "        else:\n",
    "            B, T, C = logits_gas.shape\n",
    "            logits_gas = logits_gas.view(B*T, C)\n",
    "            targets_gas = targets_gas.view(B*T)\n",
    "            loss_gas = F.cross_entropy(logits_gas, targets_gas)\n",
    "        # now the same for elec\n",
    "        if targets_elec is None:\n",
    "            loss_elec = None\n",
    "        else:\n",
    "            B, T, C = logits_elec.shape\n",
    "            logits_elec = logits_elec.view(B*T, C)\n",
    "            targets_elec = targets_elec.view(B*T)\n",
    "            loss_elec = F.cross_entropy(logits_elec, targets_elec)\n",
    "\n",
    "        return logits_gas, loss_gas, logits_elec, loss_elec\n",
    "\n",
    "    def generate(self, idx_gas, idx_elec, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond_gas = idx_gas[:, -block_size:]\n",
    "            idx_cond_elec = idx_elec[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits_gas, _, logits_elec, _ = self(idx_cond_gas, idx_cond_elec)\n",
    "            # focus only on the last time step\n",
    "            logits_gas = logits_gas[:, -1, :] # becomes (B, C)\n",
    "            logits_elec = logits_elec[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs_gas = F.softmax(logits_gas, dim=-1) # (B, C)\n",
    "            probs_elec = F.softmax(logits_elec, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next_gas = torch.multinomial(probs_gas, num_samples=1) # (B, 1)\n",
    "            idx_next_elec = torch.multinomial(probs_elec, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx_gas = torch.cat((idx_gas, idx_next_gas), dim=1) # (B, T+1)\n",
    "            idx_elec = torch.cat((idx_elec, idx_next_elec), dim=1) # (B, T+1)\n",
    "        return idx_gas, idx_elec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.069772 M parameters\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss gas \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss gas \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss elec \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss elec \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/makemore/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[1;32m      9\u001b[0m     X_gas, Y_gas, X_elec, Y_elec \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[0;32m---> 10\u001b[0m     logits_gas, loss_gas, logits_elec, loss_elec \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_gas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_elec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_gas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_elec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     losses[k,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m loss_gas\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     12\u001b[0m     losses[k,\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m loss_elec\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/repos/makemore/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/makemore/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mDualTransformerModel.forward\u001b[0;34m(self, idx_gas, idx_elec, targets_gas, targets_elec)\u001b[0m\n\u001b[1;32m     42\u001b[0m x_elec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_blocks_elec(x_elec) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# cross attention layers\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m x_gas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_blocks_gas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_gas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_elec\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m x_elec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_blocks_elec(x_elec, x_gas) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# final output layers\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/makemore/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/makemore/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "model = DualTransformerModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss gas {losses['train'][0]:.4f}, val loss gas {losses['val'][0]:.4f}\")\n",
    "        print(f\"step {iter}: train loss elec {losses['train'][1]:.4f}, val loss elec {losses['val'][1]:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb_gas, yb_gas, xb_elec, yb_elec = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits_gas, loss_gas, logits_elec, loss_elec = model(xb_gas, xb_elec, yb_gas, yb_elec)\n",
    "    # combine the losses\n",
    "    total_loss = loss_gas + loss_elec\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
